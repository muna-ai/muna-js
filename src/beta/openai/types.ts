/*
*   Muna
*   Copyright Â© 2025 NatML Inc. All Rights Reserved.
*/

export interface ChatCompletion {
    /**
     * The object type, which is always `chat.completion`.
     */
    object: "chat.completion";
    /**
     * A unique identifier for the chat completion.
     */
    id: string;
    /**
     * The model used for the chat completion.
     */
    model: string;
    /**
     * A list of chat completion choices. Can be more than one if `n` is greater
     * than 1.
     */
    choices: Array<ChatCompletion.Choice>;
    /**
     * The Unix timestamp (in seconds) of when the chat completion was created.
     */
    created: number;
    /**
     * Usage statistics for the completion request.
     */
    usage?: ChatCompletion.Usage;
}

export interface ChatCompletionChunk {
    /**
     * The object type, which is always `chat.completion.chunk`.
     */
    object: "chat.completion.chunk";
    /**
     * A unique identifier for the chat completion. Each chunk has the same ID.
     */
    id: string;
    /**
     * The model to generate the completion.
     */
    model: string;
    /**
     * A list of chat completion choices. Can contain more than one elements if `n` is
     * greater than 1. Can also be empty for the last chunk if you set
     * `stream_options: {"include_usage": true}`.
     */
    choices: Array<ChatCompletionChunk.Choice>;
    /**
     * The Unix timestamp (in seconds) of when the chat completion was created. Each
     * chunk has the same timestamp.
     */
    created: number;
    /**
     * Usage statistics for the completion request.
     */
    usage?: ChatCompletion.Usage;    
}

export namespace ChatCompletion {

    export interface Choice {
        /**
         * The index of the choice in the list of choices.
         */
        index: number;
        /**
         * A chat completion message generated by the model.
         */
        message: ChatCompletionMessage;
        /**
         * The reason the model stopped generating tokens. This will be `stop` if the model
         * hit a natural stop point or a provided stop sequence, `length` if the maximum
         * number of tokens specified in the request was reached, `content_filter` if
         * content was omitted due to a flag from our content filters, `tool_calls` if the
         * model called a tool, or `function_call` (deprecated) if the model called a
         * function.
         */
        finish_reason: "stop" | "length" | "tool_calls" | "content_filter" | "function_call";
        /**
         * Log probability information for the choice.
         */
        logprobs?: null;
    }

    export interface Usage {
        /**
         * Number of tokens in the generated completion.
         */
        completion_tokens: number;
        /**
         * Number of tokens in the prompt.
         */
        prompt_tokens: number;
        /**
         * Total number of tokens used in the request (prompt + completion).
         */
        total_tokens: number;
    }
}

export namespace ChatCompletionChunk {

    export interface Choice {
        /**
         * The index of the choice in the list of choices.
         */
        index: number;
        /**
         * A chat completion delta generated by streamed model responses.
         */
        delta: Choice.Delta;
        /**
         * The reason the model stopped generating tokens. This will be `stop` if the model
         * hit a natural stop point or a provided stop sequence, `length` if the maximum
         * number of tokens specified in the request was reached, `content_filter` if
         * content was omitted due to a flag from our content filters, `tool_calls` if the
         * model called a tool, or `function_call` (deprecated) if the model called a
         * function.
         */
        finish_reason: "stop" | "length" | "tool_calls" | "content_filter" | "function_call" | null;
        /**
         * Log probability information for the choice.
         */
        logprobs?: null;
    }

    export namespace Choice {

        export interface Delta {
            /**
             * The role of the author of this message.
             */
            role?: "developer" | "system" | "user" | "assistant" | "tool";
            /**
             * The contents of the chunk message.
             */
            content?: string | null;
        }
    }
}

export interface ChatCompletionMessage {
    /**
     * The role of the author of this message.
     */
    role: "assistant" | "user" | "system";
    /**
     * The contents of the message.
     */
    content: string | null;
}

/**
 * Represents an embedding vector returned by embedding endpoint.
 */
export interface Embedding {
    /**
     * The object type, which is always "embedding".
     */
    object: "embedding";
    /**
     * The embedding vector as number array or base64 string..
     */
    embedding: number[] | string;
    /**
     * The index of the embedding in the list of embeddings.
     */
    index: number;
}

export interface CreateEmbeddingResponse {
    /**
     * The object type, which is always "list".
     */
    object: "list";
    /**
     * The name of the model used to generate the embedding.
     */
    model: string;
    /**
     * The list of embeddings generated by the model.
     */
    data: Embedding[];
    /**
     * The usage information for the request.
     */
    usage: CreateEmbeddingResponse.Usage;
}

export namespace CreateEmbeddingResponse {

    /**
     * The usage information for the request.
     */
    export interface Usage {
        /**
         * The number of tokens used by the prompt.
         */
        prompt_tokens: number;
        /**
         * The total number of tokens used by the request.
         */
        total_tokens: number;
    }
}